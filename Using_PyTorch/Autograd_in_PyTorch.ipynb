{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee192251",
      "metadata": {
        "id": "ee192251"
      },
      "source": [
        "# LEC-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fcfa9a3",
      "metadata": {
        "id": "4fcfa9a3"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8927ff71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8927ff71",
        "outputId": "35e96182-deb1-45c3-f0b3-925c3ab48418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Doing Derivative without using 'argmax'\n",
        "def dy_dx(x):\n",
        "  return 2*x  # Equation of dy/dx\n",
        "\n",
        "dy_dx(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for derivation chain, where y=x^2 and z=sin(y), getting dz/dx is tough\n",
        "import math\n",
        "\n",
        "def dz_dx(x):\n",
        "  return 2*x*math.cos(x**2) # Equation of dz/dx\n",
        "\n",
        "dz_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Up_tjKMP5dF",
        "outputId": "a994d541-024e-49a0-9199-2bdc12ff04e0"
      },
      "id": "_Up_tjKMP5dF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.466781571308061"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Autograd"
      ],
      "metadata": {
        "id": "gSptrPYeVrZk"
      },
      "id": "gSptrPYeVrZk"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "KUwlTf25Qitb"
      },
      "id": "KUwlTf25Qitb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: y = x^2"
      ],
      "metadata": {
        "id": "VVnjKUdvXV63"
      },
      "id": "VVnjKUdvXV63"
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad -> to get derivatives later\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "print(x, y)\n",
        "y.backward() # dy/dx = 2x\n",
        "x.grad # x.grad contains dy/dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o7cDLM6Vxp7",
        "outputId": "8e18cb63-329d-491f-e35b-f51795e3a18e"
      },
      "id": "_o7cDLM6Vxp7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True) tensor(9., grad_fn=<PowBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: y = x^2, z = sin(y)"
      ],
      "metadata": {
        "id": "p7-5PuauXaYJ"
      },
      "id": "p7-5PuauXaYJ"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2 # y = x^2\n",
        "z = torch.sin(y) # z = sin(y)\n",
        "print(x, y, z)\n",
        "z.backward() # dz/dx\n",
        "x.grad # same when without using autograd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P11yIwIpWgHK",
        "outputId": "b7dd260a-cf57-4b69-9af8-259594517043"
      },
      "id": "P11yIwIpWgHK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True) tensor(9., grad_fn=<PowBackward0>) tensor(0.4121, grad_fn=<SinBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3: CGPA -> placed?"
      ],
      "metadata": {
        "id": "quqrdri9agf3"
      },
      "id": "quqrdri9agf3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Without using autograd"
      ],
      "metadata": {
        "id": "cHim1FaXas5x"
      },
      "id": "cHim1FaXas5x"
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs\n",
        "x = torch.tensor(6.7) # Input Fearure = CGPA\n",
        "y = torch.tensor(0) # True Label(binary) = Placed?\n",
        "\n",
        "w = torch.tensor(1.0) # Weight\n",
        "b = torch.tensor(0.0) # Bias\n",
        "\n",
        "# Bianry Cross-Entropy Loss for Scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "  epsilon = 1e-8  # To prevent log(0)\n",
        "  prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
        "  return -(target * torch.log(prediction) + (1-target) * torch.log(1-prediction))\n",
        "\n",
        "# Forward Pass\n",
        "z = w * x + b\n",
        "y_pred = torch.sigmoid(z)\n",
        "\n",
        "# Loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "Ws_Onvx3YQoD",
        "outputId": "d1b0a636-1da4-4d1a-9e2a-c07011cb415f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ws_Onvx3YQoD",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7012)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y) / (y_pred* (1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1-y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x # dz/dw = x\n",
        "dz_db = 1 # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n",
        "\n",
        "print(f\"Manual Gradients: dL/dw = {dL_dw}, dL/db = {dL_db}\")"
      ],
      "metadata": {
        "id": "NhVXEe0kbfKk",
        "outputId": "2e6179e5-9e2c-4fb6-da26-3c75b0dba9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NhVXEe0kbfKk",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradients: dL/dw = 6.691762447357178, dL/db = 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Using Autograd"
      ],
      "metadata": {
        "id": "GwuvnFhXrfa0"
      },
      "id": "GwuvnFhXrfa0"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "z = w * x + b\n",
        "y_pred = torch.sigmoid(z)\n",
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "dCCv9cvqrS_c",
        "outputId": "9d221a62-e723-493b-fbd6-cc8093307080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dCCv9cvqrS_c",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7012, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(f\"Manual Gradients: dL/dw = {w.grad}, dL/db = {b.grad}\")"
      ],
      "metadata": {
        "id": "Ntg5-wsasHYK",
        "outputId": "a4c403c5-fa3b-4f5c-e60d-65e3ee65f986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ntg5-wsasHYK",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradients: dL/dw = 6.6917619705200195, dL/db = 0.9987704753875732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Vector Inputs in Autograd"
      ],
      "metadata": {
        "id": "dZDjRCmKs0wK"
      },
      "id": "dZDjRCmKs0wK"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = (x**2).mean()\n",
        "y"
      ],
      "metadata": {
        "id": "7WyN28Unsk-m",
        "outputId": "70f4bc58-9270-466a-cb64-4cba60b81447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7WyN28Unsk-m",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "id": "5oi9LPLutOcE",
        "outputId": "2df83aa1-cd38-4854-a0ee-bb24a0f1d88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5oi9LPLutOcE",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clearing Gradients"
      ],
      "metadata": {
        "id": "JE95F0X8uK9C"
      },
      "id": "JE95F0X8uK9C"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "rRkI-AUItQtZ",
        "outputId": "ac074802-96fd-4ce7-8c0c-fc637ebbbb6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rRkI-AUItQtZ",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "y"
      ],
      "metadata": {
        "id": "IKz1IeTkumXz",
        "outputId": "f8438c0e-d62d-4a18-92a2-dd5f03ad6c85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IKz1IeTkumXz",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "yb6sVRJIuWD8"
      },
      "id": "yb6sVRJIuWD8",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad\n",
        "# Gradients Accumulates over running forward, backwaed, x.grad multiple times\n",
        "# Like when runned 1st x.grad is 4, then 2nd time x.grad is 8, next 12 and so\n",
        "# that is why we manually need to clear the previous grad"
      ],
      "metadata": {
        "id": "tumIsxyjubyu",
        "outputId": "f3f66887-4406-43ae-a7f8-8e4b00a61fa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tumIsxyjubyu",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_() # Will be used in each epoches after getting grad"
      ],
      "metadata": {
        "id": "EzMuWKZ7uc8j",
        "outputId": "4c5934b9-60ac-4e1d-9fc8-df153d3a3e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EzMuWKZ7uc8j",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopping Gradinent Tracking after Training"
      ],
      "metadata": {
        "id": "lFcG_JyHvtl5"
      },
      "id": "lFcG_JyHvtl5"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "id": "mPGyioSGvRLv",
        "outputId": "33fc7ee3-705b-4d8d-b1bb-e493e21b76fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mPGyioSGvRLv",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "y"
      ],
      "metadata": {
        "id": "6lZxZ5H9wAU4",
        "outputId": "3457ac64-1581-4266-ed72-04ee782c4412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6lZxZ5H9wAU4",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "SXLMuhfFwCqx"
      },
      "id": "SXLMuhfFwCqx",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "id": "ttnGCofzwJOl",
        "outputId": "eafabd51-17bc-4994-e161-cd43c67bae82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ttnGCofzwJOl",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 1 - requires_grad_(False)\n",
        "x.requires_grad_(False)\n",
        "print(x)\n",
        "# y.backward() # Can't call y.backward() anymore\n",
        "\n",
        "# option 2 - detach()\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "z = x.detach()\n",
        "y = x**2\n",
        "y1 = z**2\n",
        "print(y, y1)\n",
        "y.backward()\n",
        "# y1.backward() # Can't call y1.backward() anymore\n",
        "\n",
        "# option 3 - torch.no_grad()\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "# with torch.no_grad(): # Using with torch.no_grad() we cant call y.backward()\n",
        "#   y = x**2\n",
        "#   y.backward()\n",
        "#   print(x.grad)\n",
        "y = x**2\n",
        "y.backward()  # As we no longer have the method, we can call y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "N5LQUcMewKnq",
        "outputId": "852e30a9-a02c-4e6d-896d-8c4bee18d101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N5LQUcMewKnq",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n",
            "tensor(4., grad_fn=<PowBackward0>) tensor(4.)\n",
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SD7lp1tIwl2G"
      },
      "id": "SD7lp1tIwl2G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}